{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-19T11:16:25.282169Z",
     "start_time": "2023-09-19T11:16:25.276010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from langchain.tools import Tool, DuckDuckGoSearchResults\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:90.0) Gecko/20100101 Firefox/90.0'\n",
    "}\n",
    "\n",
    "def fetch_metrics(url: str) -> str:\n",
    "    return f\"user1 response time: 300ms, user1 response time: 900ms: ({url})\"\n",
    "\n",
    "def fetch_logs(url: str) -> str:\n",
    "    return f\"user1 request size: 100 kb, user2 request size: 7887 kb: ({url})\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T11:16:25.284616Z",
     "start_time": "2023-09-19T11:16:25.283242Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# ddg_search = DuckDuckGoSearchResults()\n",
    "\n",
    "metrics_fetch_tool = Tool.from_function(\n",
    "    func=fetch_metrics,\n",
    "    name=\"MetricFetcher\",\n",
    "    description=\"Fetches response time metrics from client calls\"\n",
    ")\n",
    "\n",
    "logs_fetch_tool = Tool.from_function(\n",
    "    func=fetch_logs,\n",
    "    name=\"LogsFetcher\",\n",
    "    description=\"Fetches logs with client request size in KB\"\n",
    ")\n",
    "\n",
    "prompt_template = \"Given response time by user and request size by user, suggest improvements for an API: {content}\"\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "\n",
    "suggester_tool = Tool.from_function(\n",
    "    func=llm_chain.run,\n",
    "    name=\"Suggester\",\n",
    "    description=\"Suggest API improvements based on metrics and logs\"\n",
    ")\n",
    "\n",
    "tools = [logs_fetch_tool, metrics_fetch_tool, suggester_tool]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T11:24:17.129329Z",
     "start_time": "2023-09-19T11:24:17.124751Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mI need to analyze the response time of client calls to identify any areas of improvement. I can use the MetricFetcher to fetch response time metrics and the LogsFetcher to fetch logs with client request size. Once I have the data, I can use the Suggester tool to suggest API improvements based on the metrics and logs.\n",
      "Action: MetricFetcher\n",
      "Action Input: Fetch response time metrics\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3muser1 response time: 300ms, user1 response time: 900ms: (Fetch response time metrics)\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3mI have fetched the response time metrics for the client calls. Now, I need to fetch the logs with client request size to get a complete understanding of the performance.\n",
      "Action: LogsFetcher\n",
      "Action Input: Fetch logs with client request size\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3muser1 request size: 100 kb, user2 request size: 7887 kb: (Fetch logs with client request size)\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3mI now have both the response time metrics and the client request size logs. I can use this data to identify any areas of improvement and suggest API improvements using the Suggester tool.\n",
      "Action: Suggester\n",
      "Action Input: Response time metrics and client request size logs\u001B[0m\n",
      "Observation: \u001B[38;5;200m\u001B[1;3mTo improve the API's response time and handle client request sizes more efficiently, you can consider the following improvements:\n",
      "\n",
      "1. Caching: Implement a caching mechanism to store frequently requested data. This can help reduce the response time for subsequent requests by serving the data directly from the cache instead of generating it again.\n",
      "\n",
      "2. Load Balancing: Utilize load balancing techniques to distribute client requests across multiple servers. This can help distribute the workload evenly and prevent any single server from being overwhelmed, improving response times.\n",
      "\n",
      "3. Compression: Implement data compression techniques such as Gzip or Brotli to reduce the size of the response payload. This can significantly decrease the time required to transfer data over the network, especially for large requests.\n",
      "\n",
      "4. Asynchronous Processing: Identify any long-running or resource-intensive tasks within the API and offload them to asynchronous processing. This allows the API to respond to client requests quickly while the heavy tasks are processed in the background.\n",
      "\n",
      "5. Query Optimization: Analyze and optimize any database queries or data retrieval operations to minimize the response time. Use appropriate indexes, query optimization techniques, or consider denormalizing data when necessary.\n",
      "\n",
      "6. API Caching: Implement an API-level caching mechanism to cache the responses of frequently requested API endpoints. This can help reduce the processing time required to generate responses for subsequent requests.\n",
      "\n",
      "7. Request Size Limitations: Analyze the average client request size and consider implementing request size limitations or chunked transfer encoding. This can prevent oversized requests from impacting the API's performance and potentially causing bottlenecks.\n",
      "\n",
      "8. API Versioning: Consider implementing versioning for your API to allow for updates and improvements without breaking existing client integrations. This ensures backward compatibility and allows clients to update at their own pace.\n",
      "\n",
      "9. Monitoring and Logging: Set up a comprehensive monitoring and logging system to track response time metrics and client request size logs. This can help identify bottlenecks, track performance trends, and proactively address any issues.\n",
      "\n",
      "10. Performance Testing: Regularly perform performance testing on the API to identify potential performance bottlenecks and areas for improvement. Load testing can help simulate real-world usage scenarios and ensure that the API can handle the expected load effectively.\n",
      "\n",
      "By implementing these improvements, you can enhance the API's response time and handle client request sizes more efficiently, resulting in a better overall user experience.\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3mBased on the analysis of response time metrics and client request size logs, I have identified several areas of improvement and suggested API improvements using the Suggester tool. The improvements include implementing caching, load balancing, compression, asynchronous processing, query optimization, API caching, request size limitations, API versioning, monitoring and logging, and performance testing. These improvements aim to enhance the API's response time and handle client request sizes more efficiently, ultimately improving the user experience.\n",
      "\n",
      "Final Answer: The proposed improvements based on the analysis of metrics and logs are: caching, load balancing, compression, asynchronous processing, query optimization, API caching, request size limitations, API versioning, monitoring and logging, and performance testing.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "The proposed improvements based on the analysis of metrics and logs are: caching, load balancing, compression, asynchronous processing, query optimization, API caching, request size limitations, API versioning, monitoring and logging, and performance testing.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Evaluate the current metrics and propose an improvements. Use your tools to fetch metrics and logs about the response time of client calls.\"\n",
    "\n",
    "print(agent.run(prompt))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T11:24:28.415475Z",
     "start_time": "2023-09-19T11:24:17.950100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mTo determine which user is experiencing longer response times and why, we need to fetch the response time metrics from client calls using the MetricFetcher tool.\n",
      "Action: MetricFetcher\n",
      "Action Input: Fetch response time metrics for all users\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3muser1 response time: 300ms, user1 response time: 900ms: (Fetch response time metrics for all users)\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3mNow that we have the response time metrics for all users, we can analyze the data to determine which user is experiencing longer response times and why. We can also generate potential hypotheses based on this information.\n",
      "Action: Analyze response time metrics\n",
      "Action Input: Response time metrics for all users\u001B[0m\n",
      "Observation: Analyze response time metrics is not a valid tool, try one of [LogsFetcher, MetricFetcher, Suggester].\n",
      "Thought:\u001B[32;1m\u001B[1;3mOops! It seems that \"Analyze response time metrics\" is not a valid tool. Let's try using the LogsFetcher to fetch logs with client request size in KB and analyze them to determine the user with longer response times and potential hypotheses.\n",
      "Action: LogsFetcher\n",
      "Action Input: Fetch logs with client request size in KB for all users\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3muser1 request size: 100 kb, user2 request size: 7887 kb: (Fetch logs with client request size in KB for all users)\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3mNow that we have fetched the logs with client request size in KB for all users, we can analyze the data to determine which user is experiencing longer response times and generate potential hypotheses based on this information.\n",
      "Action: Analyze logs\n",
      "Action Input: Logs with client request size in KB for all users\u001B[0m\n",
      "Observation: Analyze logs is not a valid tool, try one of [LogsFetcher, MetricFetcher, Suggester].\n",
      "Thought:\u001B[32;1m\u001B[1;3mOops! It seems that \"Analyze logs\" is not a valid tool either. In that case, let's try using the Suggester tool to generate potential API improvements based on the response time metrics and logs.\n",
      "Action: Suggester\n",
      "Action Input: Response time metrics and logs for all users\u001B[0m\n",
      "Observation: \u001B[38;5;200m\u001B[1;3mTo improve the API's response time metrics and logs for all users, consider the following suggestions:\n",
      "\n",
      "1. Optimize API Performance: Analyze and optimize the API's internal processes to minimize response time. This can involve database optimization, code refactoring, caching, and utilizing efficient algorithms.\n",
      "\n",
      "2. Implement Load Balancing: Distribute incoming requests across multiple servers to prevent overload and improve response time. Load balancing helps ensure that the API can handle a higher volume of requests without compromising performance.\n",
      "\n",
      "3. Monitor Server Health: Set up monitoring tools to track server health and performance metrics in real-time. This will help identify any bottlenecks or issues that could impact response time. Use tools like New Relic, Datadog, or Prometheus to monitor server health.\n",
      "\n",
      "4. Use Caching Mechanisms: Implement caching mechanisms like Redis or Memcached to store and serve frequently accessed data. Caching can significantly reduce response time by retrieving pre-computed results instead of executing time-consuming operations repeatedly.\n",
      "\n",
      "5. Employ Content Delivery Networks (CDNs): Utilize CDNs to cache and serve static content closer to the end-users. CDNs distribute content across multiple servers worldwide, reducing latency and improving response time, especially for geographically dispersed users.\n",
      "\n",
      "6. Optimize Database Queries: Analyze and optimize database queries by creating appropriate indexes, avoiding unnecessary joins, and optimizing data retrieval operations. This can significantly improve response time, especially for APIs heavily relying on database interactions.\n",
      "\n",
      "7. Implement Request Throttling: Introduce request throttling to limit the number of requests a user can make within a specific time frame. Throttling prevents abuse and ensures fair usage, ultimately improving response time for all users.\n",
      "\n",
      "8. Monitor and Analyze Logs: Implement a centralized logging system to collect and analyze API logs. This will help identify patterns, errors, and performance issues that may impact response time. Tools like ELK Stack (Elasticsearch, Logstash, and Kibana) or Splunk can assist in log analysis.\n",
      "\n",
      "9. Implement Error Tracking: Use error tracking tools like Sentry or Bugsnag to monitor and track API errors. This will help identify and resolve issues that affect response time and user experience.\n",
      "\n",
      "10. Regularly Benchmark and Test: Perform regular benchmarking and load testing to measure the API's performance under different conditions. This helps identify potential improvements and ensures the API can handle expected user load without compromising response time.\n",
      "\n",
      "By implementing these improvements, you can enhance the API's response time metrics and logs, providing a better experience for all users.\u001B[0m\n",
      "Thought:"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse LLM output: `Now that we have generated potential API improvements based on the response time metrics and logs, we can analyze them to determine which user is experiencing longer response times and potential hypotheses. Based on the observations made earlier, user2 has a significantly longer response time compared to user1. One potential hypothesis could be that user2's larger request size (7887 KB) is causing the longer response time. However, further analysis is needed to confirm this hypothesis.`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutputParserException\u001B[0m                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhich user is taking longer response time and why? Give some potential hypothesis\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/.pyenv/versions/vpyenv/lib/python3.10/site-packages/langchain/chains/base.py:487\u001B[0m, in \u001B[0;36mChain.run\u001B[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[1;32m    485\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    486\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supports only one positional argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 487\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[1;32m    488\u001B[0m         _output_key\n\u001B[1;32m    489\u001B[0m     ]\n\u001B[1;32m    491\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[1;32m    492\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(kwargs, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[1;32m    493\u001B[0m         _output_key\n\u001B[1;32m    494\u001B[0m     ]\n",
      "File \u001B[0;32m~/.pyenv/versions/vpyenv/lib/python3.10/site-packages/langchain/chains/base.py:292\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    291\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 292\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    293\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    294\u001B[0m final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    295\u001B[0m     inputs, outputs, return_only_outputs\n\u001B[1;32m    296\u001B[0m )\n",
      "File \u001B[0;32m~/.pyenv/versions/vpyenv/lib/python3.10/site-packages/langchain/chains/base.py:286\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    279\u001B[0m run_manager \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_chain_start(\n\u001B[1;32m    280\u001B[0m     dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    281\u001B[0m     inputs,\n\u001B[1;32m    282\u001B[0m     name\u001B[38;5;241m=\u001B[39mrun_name,\n\u001B[1;32m    283\u001B[0m )\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    285\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 286\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    287\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    288\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    289\u001B[0m     )\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    291\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/.pyenv/versions/vpyenv/lib/python3.10/site-packages/langchain/agents/agent.py:1039\u001B[0m, in \u001B[0;36mAgentExecutor._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;66;03m# We now enter the agent loop (until it returns something).\u001B[39;00m\n\u001B[1;32m   1038\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_continue(iterations, time_elapsed):\n\u001B[0;32m-> 1039\u001B[0m     next_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_take_next_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1040\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname_to_tool_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1041\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolor_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1042\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1043\u001B[0m \u001B[43m        \u001B[49m\u001B[43mintermediate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1044\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1045\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1046\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(next_step_output, AgentFinish):\n\u001B[1;32m   1047\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_return(\n\u001B[1;32m   1048\u001B[0m             next_step_output, intermediate_steps, run_manager\u001B[38;5;241m=\u001B[39mrun_manager\n\u001B[1;32m   1049\u001B[0m         )\n",
      "File \u001B[0;32m~/.pyenv/versions/vpyenv/lib/python3.10/site-packages/langchain/agents/agent.py:847\u001B[0m, in \u001B[0;36mAgentExecutor._take_next_step\u001B[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[0m\n\u001B[1;32m    845\u001B[0m     raise_error \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    846\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m raise_error:\n\u001B[0;32m--> 847\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    848\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(e)\n\u001B[1;32m    849\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_parsing_errors, \u001B[38;5;28mbool\u001B[39m):\n",
      "File \u001B[0;32m~/.pyenv/versions/vpyenv/lib/python3.10/site-packages/langchain/agents/agent.py:836\u001B[0m, in \u001B[0;36mAgentExecutor._take_next_step\u001B[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[0m\n\u001B[1;32m    833\u001B[0m     intermediate_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_intermediate_steps(intermediate_steps)\n\u001B[1;32m    835\u001B[0m     \u001B[38;5;66;03m# Call the LLM to see what to do.\u001B[39;00m\n\u001B[0;32m--> 836\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplan\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    837\u001B[0m \u001B[43m        \u001B[49m\u001B[43mintermediate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    838\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    839\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    840\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    841\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m OutputParserException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    842\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_parsing_errors, \u001B[38;5;28mbool\u001B[39m):\n",
      "File \u001B[0;32m~/.pyenv/versions/vpyenv/lib/python3.10/site-packages/langchain/agents/agent.py:457\u001B[0m, in \u001B[0;36mAgent.plan\u001B[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    455\u001B[0m full_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_full_inputs(intermediate_steps, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    456\u001B[0m full_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_chain\u001B[38;5;241m.\u001B[39mpredict(callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfull_inputs)\n\u001B[0;32m--> 457\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_parser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfull_output\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/vpyenv/lib/python3.10/site-packages/langchain/agents/mrkl/output_parser.py:52\u001B[0m, in \u001B[0;36mMRKLOutputParser.parse\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m AgentFinish(\n\u001B[1;32m     48\u001B[0m         {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput\u001B[39m\u001B[38;5;124m\"\u001B[39m: text\u001B[38;5;241m.\u001B[39msplit(FINAL_ANSWER_ACTION)[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mstrip()}, text\n\u001B[1;32m     49\u001B[0m     )\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m re\u001B[38;5;241m.\u001B[39msearch(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAction\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms*\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md*\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms*:[\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms]*(.*?)\u001B[39m\u001B[38;5;124m\"\u001B[39m, text, re\u001B[38;5;241m.\u001B[39mDOTALL):\n\u001B[0;32m---> 52\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m OutputParserException(\n\u001B[1;32m     53\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not parse LLM output: `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtext\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     54\u001B[0m         observation\u001B[38;5;241m=\u001B[39mMISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE,\n\u001B[1;32m     55\u001B[0m         llm_output\u001B[38;5;241m=\u001B[39mtext,\n\u001B[1;32m     56\u001B[0m         send_to_llm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     57\u001B[0m     )\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m re\u001B[38;5;241m.\u001B[39msearch(\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms]*Action\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms*\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md*\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms*Input\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms*\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md*\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms*:[\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms]*(.*)\u001B[39m\u001B[38;5;124m\"\u001B[39m, text, re\u001B[38;5;241m.\u001B[39mDOTALL\n\u001B[1;32m     60\u001B[0m ):\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m OutputParserException(\n\u001B[1;32m     62\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not parse LLM output: `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtext\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     63\u001B[0m         observation\u001B[38;5;241m=\u001B[39mMISSING_ACTION_INPUT_AFTER_ACTION_ERROR_MESSAGE,\n\u001B[1;32m     64\u001B[0m         llm_output\u001B[38;5;241m=\u001B[39mtext,\n\u001B[1;32m     65\u001B[0m         send_to_llm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     66\u001B[0m     )\n",
      "\u001B[0;31mOutputParserException\u001B[0m: Could not parse LLM output: `Now that we have generated potential API improvements based on the response time metrics and logs, we can analyze them to determine which user is experiencing longer response times and potential hypotheses. Based on the observations made earlier, user2 has a significantly longer response time compared to user1. One potential hypothesis could be that user2's larger request size (7887 KB) is causing the longer response time. However, further analysis is needed to confirm this hypothesis.`"
     ]
    }
   ],
   "source": [
    "prompt = \"Which user is taking longer response time and why? Give some potential hypothesis\"\n",
    "\n",
    "print(agent.run(prompt))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T11:25:49.793445Z",
     "start_time": "2023-09-19T11:25:35.283343Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# planner = load_chat_planner(llm)\n",
    "# executor = load_agent_executor(llm, tools, verbose=True)\n",
    "# agent = PlanAndExecute(planner=planner, executor=executor, verbose=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T11:16:28.602326Z",
     "start_time": "2023-09-19T11:16:28.599019Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# result = agent.run(\"Research how to use the requests library in Python. Use your tools to search and summarize content into a guide on how to use the requests library.\")\n",
    "# print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T11:16:28.607084Z",
     "start_time": "2023-09-19T11:16:28.603585Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T11:16:28.609175Z",
     "start_time": "2023-09-19T11:16:28.607401Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
